use std::fs;
use std::io::{self, Write};
use std::fmt;
use std::iter::Peekable;
use std::str::Chars;
#[derive(Debug, Clone, PartialEq)]
#[allow(non_camel_case_types)]

pub enum TokenType {
    // Single-character tokens.
    LEFT_PAREN,
    RIGHT_PAREN,
    LEFT_BRACE,
    RIGHT_BRACE,
    COMMA,
    DOT,
    MINUS,
    PLUS,
    SEMICOLON,
    SLASH,
    STAR,

    // One or two character tokens.
    BANG,
    BANG_EQUAL,
    EQUAL,
    EQUAL_EQUAL,
    GREATER,
    GREATER_EQUAL,
    LESS,
    LESS_EQUAL,

    // Literals.
    IDENTIFIER,
    STRING,
    NUMBER,

    // Keywords.
    AND,
    CLASS,
    ELSE,
    FALSE,
    FUN,
    FOR,
    IF,
    NIL,
    OR,
    PRINT,
    RETURN,
    SUPER,
    THIS,
    TRUE,
    VAR,
    WHILE,

    EOF,
}

fn keywords(key: &str) -> Option<TokenType> {
    match key {
        "and" => Some(TokenType::AND),
        "class" => Some(TokenType::CLASS),
        "else" => Some(TokenType::ELSE),
        "false" => Some(TokenType::FALSE),
        "for" => Some(TokenType::FOR),
        "fun" => Some(TokenType::FUN),
        "if" => Some(TokenType::IF),
        "nil" => Some(TokenType::NIL),
        "or" => Some(TokenType::OR),
        "print" => Some(TokenType::PRINT),
        "return" => Some(TokenType::RETURN),
        "super" => Some(TokenType::SUPER),
        "this" => Some(TokenType::THIS),
        "true" => Some(TokenType::TRUE),
        "var" => Some(TokenType::VAR),
        "while" => Some(TokenType::WHILE),
        _ => None,
    }
}

#[derive(Debug, Clone, PartialEq)]
pub enum Literal {
    Number(f64),
    String(String),
    Boolean(bool),
    Identifier(String),
    None,
}

impl fmt::Display for Literal {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Literal::Boolean(value) => write!(f, "{}", value),
            Literal::String(value) => write!(f, "{}", value),
            Literal::Number(value) => write!(f, "{:?}", value),
            Literal::Identifier(value) => write!(f, "{}", value),
            Literal::None => write!(f, "null"),
        }
    }
}

#[derive(Debug, Clone, PartialEq)]
pub struct Token {
    pub token_type: TokenType,
    pub lexeme: String,
    pub line: usize,
    pub literal: Literal,
}

fn to_string(token: Token) -> String {
    format!("{:?} {} {}", token.token_type, token.lexeme, token.literal)
}

struct Lexer {
    tokens: Vec<Token>,
    had_error: bool,
    line: usize,
}

impl Lexer {
    pub fn new() -> Self {
        Lexer {
            tokens: Vec::new(),
            had_error: false,
            line: 1,
        }
    }

    pub fn error(&mut self, line: usize, message: &str) {
        self.report(line, "", message);
    }

    pub fn report(&mut self, line: usize, location: &str, message: &str) {
        eprintln!("[line {}] Error{}: {}", line, location, message);
        self.had_error = true;
    }

    fn add_token(&mut self, token_type: TokenType, current: String) {
        self.add_token_literal(token_type, current, Literal::None);
    }
    
    fn add_token_literal(&mut self,  token_type: TokenType, current: String, literal: Literal) {
        self.tokens.push(Token {
            token_type,
            lexeme: current,
            line: self.line,
            literal,
        })
    }

    fn match_next(
        &mut self,
        chars: &mut Peekable<Chars>,
        current: char,
        expected: char,
        double_type: TokenType,
        single_type: TokenType,
    ) {
        let (token_type, lexeme) = if chars.peek() == Some(&expected) {
            chars.next();
            (double_type, format!("{}{}", current, expected))
        } else {
            (single_type, current.to_string())
        };
        self.add_token(token_type, lexeme);
    }

    fn handle_slash(&mut self, chars: &mut Peekable<Chars>) {
        if let Some(&'/') = chars.peek() {
            while chars.peek().map_or(false, |&c| c != '\n') {
                chars.next();
            }
        } else {
            self.add_token(TokenType::SLASH, '/'.to_string());
        }
    }

    fn scan_string(&mut self, chars: &mut Peekable<Chars>) {
        let mut value = String::new();
        while chars.peek().is_some() {
            if let Some(ch) = chars.peek() {
                match ch {
                    '"' => {
                        chars.next();
                        return self.add_token_literal(TokenType::STRING, format!("\"{}\"", value), Literal::String(value.clone()));
                    }
                    '\n' => {
                        self.line += 1;
                        value.push(*ch);
                    }
                    _ => value.push(*ch),
                }
            }
            chars.next();
        }
        self.error(self.line, "Unterminated string.");
    }

    fn scan_num(&mut self, chars: &mut Peekable<Chars>, cur: char) {
        let mut value = String::from(cur);
        while chars.peek().is_some() {
            if let Some(digit) = chars.peek() {
                match digit {
                    '0'..='9' => value.push(*digit),
                    '.' => value.push(*digit),
                    _ => break,
                }
            }
            chars.next(); 
        }
        let num = value.parse::<f64>().unwrap();
        self.add_token_literal(TokenType::NUMBER, value, Literal::Number(num));
    }

    fn scan_identifier(
        &mut self,
        chars: &mut Peekable<Chars>,
        cur: char,
    ) {
        let mut identifier = String::from(cur);
        while let Some(ch) = chars.peek() {
            match ch {
                ch if ch.is_alphanumeric() || ch == &'_' => identifier.push(*ch),
                _ => break,
            }
            chars.next();
        }

        if let Some(reserved) = keywords(&identifier) {
            self.add_token(reserved, identifier);
        } else {
            self.add_token(TokenType::IDENTIFIER, identifier);
        }
    }

    pub fn scan_token(&mut self, source: &str) {
        let mut chars = source.chars().peekable();

        while let Some(current) = chars.next() {
            match current {
                '(' => self.add_token(TokenType::LEFT_PAREN, current.to_string()),
                ')' => self.add_token(TokenType::RIGHT_PAREN, current.to_string()),
                '{' => self.add_token(TokenType::LEFT_BRACE, current.to_string()),
                '}' => self.add_token(TokenType::RIGHT_BRACE, current.to_string()),
                ',' => self.add_token(TokenType::COMMA, current.to_string()),
                '.' => self.add_token(TokenType::DOT, current.to_string()),
                '-' => self.add_token(TokenType::MINUS, current.to_string()),
                '+' => self.add_token(TokenType::PLUS, current.to_string()),
                ';' => self.add_token(TokenType::SEMICOLON, current.to_string()),
                '*' => self.add_token(TokenType::STAR, current.to_string()),
                '!' => self.match_next(&mut chars, current, '=', TokenType::BANG_EQUAL, TokenType::BANG),
                '=' => self.match_next(&mut chars, current, '=', TokenType::EQUAL_EQUAL, TokenType::EQUAL),
                '<' => self.match_next(&mut chars, current, '=', TokenType::LESS_EQUAL, TokenType::LESS),
                '>' => self.match_next(&mut chars, current, '=', TokenType::GREATER_EQUAL, TokenType::GREATER),
                '/' => self.handle_slash(&mut chars),
                '"' => self.scan_string(&mut chars),
                '0'..='9' => self.scan_num(&mut chars, current),
                id if id == '_' || id.is_alphanumeric() => self.scan_identifier(&mut chars, current),
                _ => {
                    // Handle unexpected characters
                    if current.is_whitespace() {
                        if current == '\n' {
                            self.line += 1;
                        }
                    } else {
                        self.error(self.line, &format!("Unexpected character: {}", current));
                    }
                }
            }
        }
        self.add_token(TokenType::EOF, "".to_string());
    }

    pub fn lex(&mut self, filename: &str) {
        let file_contents = match fs::read_to_string(filename) {
            Ok(contents) => contents,
            Err(_) => {
                writeln!(io::stderr(), "Failed to read file {}", filename).unwrap();
                return;
            }
        };
    
        if file_contents.is_empty() {
            println!("EOF  null");
            return;
        }
        self.scan_token(&file_contents);
        for token in &self.tokens {
            println!("{}", to_string(token.clone()));
        }

        if self.had_error {
            std::process::exit(65)
        } 
        std::process::exit(0)
    }
}

pub fn return_tokens(source: &str) -> Vec<Token> {
    let mut lexer = Lexer::new();
    lexer.scan_token(source);
    if lexer.had_error {
        std::process::exit(65)
    }
    lexer.tokens
}

pub fn run_lexer(filename: &str) {
    let mut lexer = Lexer::new();
    lexer.lex(filename);
}
